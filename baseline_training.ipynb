{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparation of Baseline and RL Models and Hyperparameter Tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries\n",
    "   \n",
    "2. Tunning of the Taxi Env\n",
    "   1. Grid Search\n",
    "   2. Multiprocessing\n",
    "   \n",
    "3. PPO tunning - Grid Search\n",
    "   \n",
    "4. PPO training\n",
    "   \n",
    "5. A2C tunning - Grid Search\n",
    "\n",
    "6. A2C training\n",
    "\n",
    "7. DQN tunning - Grid Search\n",
    "\n",
    "8. DQN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DQN\n",
    "import pickle\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Model evaluation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# hyperameters tuning\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Wrappers \n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium import RewardWrapper, ActionWrapper\n",
    "\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Taxi-v3 environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_baseline= gym.make('Taxi-v3', render_mode= None)\n",
    "print(type(env_baseline))\n",
    "\n",
    "obs, info = env_baseline.reset()\n",
    "print(env_baseline.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi-v3 environment with DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env = TimeLimit(env, max_episode_steps=200)\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters Tuning definition - GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def param_tuning(model_class, env, params, total_timesteps=1000000, n_eval_episodes=10, verbose=0):\n",
    "\n",
    "#     best_reward = -float('inf')\n",
    "#     best_params = None\n",
    "#     param_grid = ParameterGrid(params)\n",
    "#     for param_comb in param_grid:\n",
    "#         print(f\"Testing with parameter combination: {param_comb}\")\n",
    "#         model = model_class(\"MlpPolicy\", env, param_comb, verbose=verbose)\n",
    "#         model.learn(total_timesteps=total_timesteps)\n",
    "#         mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "#         if mean_reward > best_reward:\n",
    "#             best_reward = mean_reward\n",
    "#             best_params = param_comb\n",
    "\n",
    "#     return best_params, best_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters Tuning using Multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env = TimeLimit(env, max_episode_steps=200)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(model_class, param_comb, total_timesteps, n_eval_episodes, verbose):\n",
    "    print(f\"Testing with parameters: {param_comb}\")\n",
    "    env = env_fn()  # Cria uma nova instância do ambiente para cada thread\n",
    "    model = model_class(\"MlpPolicy\", env, **param_comb, verbose=verbose)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "    print(f\"Mean reward: {mean_reward} with parameters: {param_comb}\")\n",
    "    return mean_reward, param_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning_multithread(model_class, params, total_timesteps, n_eval_episodes, verbose, max_threads):\n",
    "\n",
    "    param_grid = list(ParameterGrid(params))  \n",
    "    best_reward = -float('inf')\n",
    "    best_params = None\n",
    "\n",
    "   \n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(evaluate_params, model_class, param_comb, total_timesteps, n_eval_episodes, verbose)\n",
    "            for param_comb in param_grid\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                mean_reward, param_comb = future.result()\n",
    "                if mean_reward > best_reward:\n",
    "                    best_reward = mean_reward\n",
    "                    best_params = param_comb\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar parâmetros: {e}\")\n",
    "\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print (f\"Best reward: {best_reward}\")\n",
    "    return best_params, best_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward Callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback que apenas adiciona recompensas ao TensorBoard,\n",
    "    sem sobrescrever os gráficos padrão do Stable Baselines3.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.episode_rewards = 0\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Acumula recompensas do episódio\n",
    "        self.episode_rewards += self.locals[\"rewards\"][0]\n",
    "\n",
    "        # Se o episódio termina, loga a recompensa acumulada\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.episode_count += 1\n",
    "            self.writer.add_scalar(\"reward/episode_reward\", self.episode_rewards, self.episode_count)\n",
    "            self.episode_rewards = 0  # Reseta a recompensa\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Fecha o writer ao final do treinamento\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PPO - Proximal Policy Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tunning - Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ppo_grid = {\n",
    "\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2],\n",
    "    'n_steps': [128, 256, 512],\n",
    "    'batch_size': [32, 64],\n",
    "    'ent_coef': [0.0, 0.01, 0.1],\n",
    "    'gamma': [0.99, 0.95]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_reward = param_tuning_multithread(PPO, params_ppo_grid, total_timesteps=100000, n_eval_episodes=10, verbose=0, max_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_ppo =  {\n",
    "    'batch_size': 32, \n",
    "    'ent_coef': 0.1, 'gamma': 0.95, \n",
    "    'learning_rate': 0.001, \n",
    "    'n_steps': 512\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PPO Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./ppo_tensorboard/\"\n",
    "model = PPO(\"MlpPolicy\", env_baseline, **best_parameters_ppo, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = RewardLoggingCallback(log_dir=log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"ppo_taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A2C - Advantage Actor-Critic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tunning - Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_a2c_grid = {\n",
    "\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2],\n",
    "    'n_steps': [20, 50],\n",
    "    'gamma': [0.99, 0.95],\n",
    "    'ent_coef': [0.0, 0.01, 0.1],\n",
    "    'vf_coef': [0.5, 0.75],\n",
    "    'max_grad_norm': [1.0, 2.0],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_a2c, best_reward_a2c = param_tuning_multithread(A2C,params_a2c_grid,total_timesteps=100000,n_eval_episodes=10,verbose=0,max_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_a2c = {\n",
    "    'ent_coef': 0.0, \n",
    "    'gamma': 0.99, \n",
    "    'learning_rate': 0.0001, \n",
    "    'max_grad_norm': 1.0, \n",
    "    'n_steps': 50, \n",
    "    'vf_coef': 0.75\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A2C Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./a2c_tensorboard/\"\n",
    "model = A2C(\"MlpPolicy\", env_baseline, **best_parameters_a2c, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = RewardLoggingCallback(log_dir=log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"a2c_taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN - Deep Q-Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tunning - Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dqn_grid = {\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2],        # Learning rate for the optimizer\n",
    "    'buffer_size': [50000, 100000],            # Replay buffer size\n",
    "    'batch_size': [32, 64],                    # Mini-batch size for training\n",
    "    'gamma': [0.95, 0.99],                     # Discount factor for future rewards\n",
    "    'exploration_final_eps': [0.01, 0.1],      # Final value of epsilon for exploration\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_dqn, best_reward_dqn = param_tuning_multithread(DQN, params_dqn_grid, total_timesteps=100000, n_eval_episodes=10, verbose=0, max_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_dqn = {\n",
    "    'batch_size': 32, \n",
    "    'buffer_size': 100000, \n",
    "    'exploration_final_eps': 0.1, \n",
    "    'gamma': 0.95, \n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dqn = gym.make(\"Taxi-v3\")\n",
    "env_dqn = TimeLimit(env_dqn, max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./dqn_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env_dqn, **best_parameters_dqn, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = RewardLoggingCallback(log_dir=log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"dqn_taxi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-taxi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
