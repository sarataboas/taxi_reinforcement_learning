{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Changes in environment**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries\n",
    "\n",
    "2. Modify Actions\n",
    "   1. Evaluate with PPO\n",
    "   2. Evaluate with A2C\n",
    "   3. Evaluate with DQN\n",
    "   \n",
    "3. Modify Rewards \n",
    "   1. Evaluate with PPO\n",
    "   2. Evaluate with A2C\n",
    "   3. Evaluate with DQN\n",
    "   \n",
    "4. Combining both changes \n",
    "   1. Evaluate with PPO\n",
    "   2. Evaluate with A2C\n",
    "   3. Evaluate with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DQN\n",
    "import pickle\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Model evaluation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy \n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# hyperameters tuning\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Wrappers \n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium import RewardWrapper, ActionWrapper\n",
    "\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modify Action Space**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first modification to the environment will be to add the ability to move diagonally to the actions. This could allow the taxi to move faster, as each diagonal action replaces two normal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActionWrapper(ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.action_space = gym.spaces.Discrete(env.action_space.n + 4) # Adicionar 4 ações ao Espaço de Ações \n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\"Transform the custom action into a base action if necessary.\"\"\"\n",
    "        if action in [6, 7, 8, 9]:\n",
    "            # Diagonal actions are handled in the `step` method.\n",
    "            return None  # Indicate custom handling\n",
    "        return action  # Pass through for base actions\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        grid_size = 5\n",
    "\n",
    "        # Get the taxi position\n",
    "        encoded_state = self.env.unwrapped.s\n",
    "        \n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.env.unwrapped.decode(encoded_state)\n",
    "\n",
    "        # Handle diagonal actions\n",
    "        if action == 6:  # Move South-East\n",
    "            new_row = min(taxi_row + 1, grid_size - 1)\n",
    "            new_col = min(taxi_col + 1, grid_size - 1)\n",
    "        elif action == 7:  # Move South-West\n",
    "            new_row = min(taxi_row + 1, grid_size - 1)\n",
    "            new_col = max(taxi_col - 1, 0)\n",
    "        elif action == 8:  # Move North-East\n",
    "            new_row = max(taxi_row - 1, 0)\n",
    "            new_col = min(taxi_col + 1, grid_size - 1)\n",
    "        elif action == 9:  # Move North-West\n",
    "            new_row = max(taxi_row - 1, 0)\n",
    "            new_col = max(taxi_col - 1, 0)\n",
    "        else:\n",
    "            # Pass through base actions to the original step method\n",
    "            return super().step(action)\n",
    "\n",
    "        # Validate the move (check for walls or invalid spaces)\n",
    "        if self.env.unwrapped.desc[new_row, new_col] != b' ':  # Assume `b' '` indicates valid space\n",
    "            # Invalid move, no state change\n",
    "            reward = -1  # Same as base environment for invalid moves\n",
    "            done = False\n",
    "            obs = self.env.unwrapped.s\n",
    "        else:\n",
    "            # Update the state manually for diagonal actions\n",
    "            self.env.unwrapped.s = self.env.unwrapped.encode(new_row, new_col, pass_idx, dest_idx)\n",
    "\n",
    "            # Compute reward manually\n",
    "            reward = -1  # Default reward for non-goal moves\n",
    "\n",
    "            # Check if the new state is terminal\n",
    "            done = self.env.unwrapped.s == self.env.unwrapped.encode(\n",
    "                dest_idx // grid_size, dest_idx % grid_size, pass_idx, dest_idx\n",
    "            )\n",
    "\n",
    "            # Get updated observation\n",
    "            obs = self.env.unwrapped.s\n",
    "\n",
    "        truncated = False  # Taxi environment doesn't use truncation\n",
    "\n",
    "        # Return updated information\n",
    "        return obs, reward, done, truncated, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the environment and modify the Action Space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_action = gym.make('Taxi-v3')\n",
    "env_custom_action = CustomActionWrapper(env_custom_action) #Adicionar as alterções ao ambiente \n",
    "env_custom_action = TimeLimit(env_custom_action,max_episode_steps=200)\n",
    "env_custom_action = DummyVecEnv([lambda: env_custom_action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback to have acess to the rewards on tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Rewards(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback que apenas adiciona recompensas ao TensorBoard,\n",
    "    sem sobrescrever os gráficos padrão do Stable Baselines3.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.episode_rewards = 0\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Acumula recompensas do episódio\n",
    "        self.episode_rewards += self.locals[\"rewards\"][0]\n",
    "\n",
    "        # Se o episódio termina, loga a recompensa acumulada\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.episode_count += 1\n",
    "            self.writer.add_scalar(\"reward/episode_reward\", self.episode_rewards, self.episode_count)\n",
    "            self.episode_rewards = 0  # Reseta a recompensa\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Fecha o writer ao final do treinamento\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's run the environment with PPO, where the best parameters were defined on tunning (It is on the `algorithms_baseline_env.ipynb`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_ppo =  {'batch_size': 32, 'ent_coef': 0.1, 'gamma': 0.95, 'learning_rate': 0.001, 'n_steps': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./ppo_custom_action_tensorboard/\"\n",
    "model = PPO(\"MlpPolicy\", env_custom_action, **best_parameters_ppo, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"ppo_taxi_custom_action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the same for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_a2c = {'ent_coef': 0.0, 'gamma': 0.99, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'n_steps': 50, 'vf_coef': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./a2c_custom_action_tensorboard/\"\n",
    "model = A2C(\"MlpPolicy\", env_custom_action, **best_parameters_a2c, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"a2c_taxi_custom_action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, testing with DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_action = gym.make('Taxi-v3')\n",
    "\n",
    "env_custom_action = CustomActionWrapper(env_custom_action) #Adicionar as alterções ao ambiente \n",
    "\n",
    "env_custom_action = TimeLimit(env_custom_action,max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_dqn = {'batch_size': 32, 'buffer_size': 100000, 'exploration_final_eps': 0.1, 'gamma': 0.95, 'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./dqn_custom_action_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env_custom_action, **best_parameters_dqn, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"dqn_taxi_custom_action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modify Rewards**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper modifies the reward system to discourage the agent (taxi) from revisiting previously visited positions unless necessary. It promotes exploration and more efficient paths by applying penalties for redundant moves. Additionally, it clears the history of visited positions after completing tasks like pickups, allowing the agent to focus on subsequent goals without unnecessary penalties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(RewardWrapper):\n",
    "    def __init__(self, env, penalty=1.0):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.visited_positions = set()  # Set to store visited positions\n",
    "        self.penalty = penalty\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, _, info = self.env.step(action)\n",
    "        \n",
    "        \n",
    "        # Get the taxi position\n",
    "        encoded_state = self.env.unwrapped.s\n",
    "        \n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.env.unwrapped.decode(encoded_state)\n",
    "        # Get the taxi's position (assuming obs contains the position as (x, y) coordinates)\n",
    "        taxi_pos = (taxi_row, taxi_col)  # The position is stored in the first two elements of the observation\n",
    "\n",
    "        # Check if the agent revisits a position\n",
    "        if taxi_pos in self.visited_positions:\n",
    "            # Apply a penalty if the taxi revisits a position\n",
    "            reward -= self.penalty\n",
    "        else:\n",
    "           \n",
    "            # Mark the current position as visited\n",
    "            self.visited_positions.add(taxi_pos)\n",
    "\n",
    "        # If the agent successfully picks up or drops off a passenger, we don't penalize\n",
    "        if done and 'pickup' in info and info['pickup']:\n",
    "            self.visited_positions.clear()  # Reset visited positions after task completion\n",
    "\n",
    "        return obs, reward, done, _, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_reward = gym.make('Taxi-v3')\n",
    "\n",
    "env_custom_reward = CustomRewardWrapper(env_custom_reward) #Adicionar as alterções ao ambiente \n",
    "\n",
    "env_custom_reward= TimeLimit(env_custom_reward,max_episode_steps=200)\n",
    "\n",
    "env_custom_reward = DummyVecEnv([lambda: env_custom_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_ppo =  {'batch_size': 32, 'ent_coef': 0.1, 'gamma': 0.95, 'learning_rate': 0.001, 'n_steps': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./ppo_custom_rewards_tensorboard/\"\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env_custom_reward, **best_parameters_ppo, verbose=0, tensorboard_log=log_dir)\n",
    "\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "\n",
    "model.save(\"ppo_taxi_custom_rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A2C**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_a2c = {'ent_coef': 0.0, 'gamma': 0.99, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'n_steps': 50, 'vf_coef': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./a2c_custom_rewards_tensorboard/\"\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env_custom_reward, **best_parameters_a2c, verbose=0, tensorboard_log=log_dir)\n",
    "\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "\n",
    "model.save(\"a2c_taxi_custom_rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_reward = gym.make('Taxi-v3')\n",
    "\n",
    "env_custom_reward = CustomRewardWrapper(env_custom_reward) #Adicionar as alterções ao ambiente \n",
    "\n",
    "env_custom_reward = TimeLimit(env_custom_reward,max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_dqn = {'batch_size': 32, 'buffer_size': 100000, 'exploration_final_eps': 0.1, 'gamma': 0.95, 'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./dqn_custom_reward_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env_custom_reward, **best_parameters_dqn, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"dqn_taxi_custom_rewards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modify Reward plus Action**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try using the two modification together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_combination = gym.make('Taxi-v3')\n",
    "env_custom_combination = CustomActionWrapper(env_custom_combination)\n",
    "env_custom_combination = CustomRewardWrapper(env_custom_combination)\n",
    "env_custom_combination = TimeLimit(env_custom_combination,max_episode_steps=200)\n",
    "env_custom_combination = DummyVecEnv([lambda: env_custom_combination])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_ppo =  {'batch_size': 32, 'ent_coef': 0.1, 'gamma': 0.95, 'learning_rate': 0.001, 'n_steps': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./ppo_custom_combination_tensorboard/\"\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env_custom_combination, **best_parameters_ppo, verbose=0, tensorboard_log=log_dir)\n",
    "\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "\n",
    "model.save(\"ppo_taxi_custom_combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A2C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_combination = gym.make('Taxi-v3')\n",
    "env_custom_combination = CustomActionWrapper(env_custom_combination)\n",
    "env_custom_combination = CustomRewardWrapper(env_custom_combination)\n",
    "env_custom_combination = TimeLimit(env_custom_combination,max_episode_steps=200)\n",
    "env_custom_combination = DummyVecEnv([lambda: env_custom_combination])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_a2c = {'ent_coef': 0.0, 'gamma': 0.99, 'learning_rate': 0.0001, 'max_grad_norm': 1.0, 'n_steps': 50, 'vf_coef': 0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./a2c_custom_combination_tensorboard/\"\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env_custom_combination, **best_parameters_a2c, verbose=0, tensorboard_log=log_dir)\n",
    "\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "\n",
    "model.save(\"a2c_taxi_custom_combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_combination = gym.make('Taxi-v3')\n",
    "env_custom_combination = CustomActionWrapper(env_custom_combination)\n",
    "env_custom_combination = CustomRewardWrapper(env_custom_combination)\n",
    "env_custom_combination = TimeLimit(env_custom_combination,max_episode_steps=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters_dqn = {'batch_size': 32, 'buffer_size': 100000, 'exploration_final_eps': 0.1, 'gamma': 0.95, 'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./dqn_custom_combination_tensorboard/\"\n",
    "model = DQN(\"MlpPolicy\", env_custom_combination, **best_parameters_dqn, verbose=0, tensorboard_log=log_dir)\n",
    "reward_callback = Rewards(log_dir= log_dir)\n",
    "model.learn(total_timesteps=40_000_000, callback=reward_callback)\n",
    "model.save(\"dqn_taxi_custom_combination\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isla2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
